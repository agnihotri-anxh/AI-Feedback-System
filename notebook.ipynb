{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 51,
            "id": "2cd5b566",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import json\n",
                "import time\n",
                "import re\n",
                "import dotenv\n",
                "import os"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "id": "ac30516c",
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(\"yelp.csv\") "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "id": "d1756e61",
            "metadata": {},
            "outputs": [],
            "source": [
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "id": "533ee906",
            "metadata": {},
            "outputs": [],
            "source": [
                "df.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "id": "b334ba4a",
            "metadata": {},
            "outputs": [],
            "source": [
                "df.duplicated().sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "id": "a7cee51c",
            "metadata": {},
            "outputs": [],
            "source": [
                "df['stars'].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "id": "f7fd41cc",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_task = df[[\"text\", \"stars\"]].dropna()\n",
                "df_task = df_task.sample(n=200, random_state=42).reset_index(drop=True)\n",
                "df_task.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "id": "0b134836",
            "metadata": {},
            "outputs": [],
            "source": [
                "dotenv.load_dotenv()\n",
                "from openai import OpenAI\n",
                "import os\n",
                "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\") \n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "sub_prompt_strategies",
            "metadata": {},
            "source": [
                "## Prompt Engineering Strategies\n",
                "\n",
                "To satisfy the task requirements, we have designed three distinct prompting approaches to evaluate performance:\n",
                "\n",
                "1. **Prompt V1 (Baseline)**: A direct, **zero-shot** instruction. This represents the simplest approach, establishing a baseline to see how well the model understands the task with minimal guidance.\n",
                "2. **Prompt V2 (Few-Shot)**: Adds **context and examples** (one negative, one mixed, one positive). This strategy aims to \"calibrate\" the model to the specific nuances of Yelp reviews and the JSON output format, theoretically improving consistency.\n",
                "3. **Prompt V3 (Strict/Reasoning)**: Implements constraints and a **Chain-of-Thought** style directive. By forcing the model to internally identify aspects (service, food) before rating, we aim to better handle complex reviews that might have conflicting sentiments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "id": "baddc840",
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt_v1  = \"\"\"\n",
                "You are a sentiment analysis API.\n",
                "\n",
                "Classify the following Yelp review into a star rating from 1 to 5.\n",
                "\n",
                "Return ONLY a valid JSON object in the following format:\n",
                "{{\n",
                "  \"predicted_stars\": <integer>,\n",
                "  \"explanation\": \"<short reasoning>\"\n",
                "}}\n",
                "\n",
                "Do NOT include markdown, backticks, or any text outside the JSON.\n",
                "\n",
                "Review:\n",
                "{review_text}\n",
                "\"\"\"\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "id": "2ae30d4d",
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt_v2 = \"\"\"\n",
                "Determine the star rating (1 to 5) for the Yelp review below.\n",
                "\n",
                "Use the following examples as a guide:\n",
                "\n",
                "Input: \"Horrible service, waiter was rude and food was cold.\"\n",
                "Output: {{ \"predicted_stars\": 1, \"explanation\": \"Negative complaints about both service and food quality.\" }}\n",
                "\n",
                "Input: \"It was okay. The burger was good but the fries were soggy.\"\n",
                "Output: {{ \"predicted_stars\": 3, \"explanation\": \"Mixed experience with both positive and negative elements.\" }}\n",
                "\n",
                "Input: \"Absolutely loved it! Best pasta I've ever had.\"\n",
                "Output: {{ \"predicted_stars\": 5, \"explanation\": \"Enthusiastic praise with no complaints.\" }}\n",
                "\n",
                "Now classify the following review.\n",
                "\n",
                "Return ONLY a valid JSON object.\n",
                "Do NOT include markdown, backticks, or any text outside the JSON.\n",
                "\n",
                "Format:\n",
                "{{\n",
                "  \"predicted_stars\": <integer>,\n",
                "  \"explanation\": \"<short reasoning>\"\n",
                "}}\n",
                "\n",
                "Review:\n",
                "{review_text}\n",
                "\"\"\"\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "id": "1ca80cdb",
            "metadata": {},
            "outputs": [],
            "source": [
                "prompt_v3 = \"\"\"\n",
                "You are a strict classification system for Yelp reviews.\n",
                "\n",
                "Internally:\n",
                "- Identify key aspects (service, food, price, etc.)\n",
                "- Assess sentiment for each aspect\n",
                "- Weigh positives and negatives to select the final rating\n",
                "\n",
                "Do NOT include internal reasoning in the output.\n",
                "\n",
                "Return ONLY valid JSON in exactly this format:\n",
                "{{\n",
                "  \"predicted_stars\": <integer between 1 and 5>,\n",
                "  \"explanation\": \"<brief justification>\"\n",
                "}}\n",
                "\n",
                "IMPORTANT:\n",
                "- Do NOT include markdown, backticks, or extra text.\n",
                "\n",
                "Review:\n",
                "{review_text}\n",
                "\"\"\"\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "id": "ddf71d6d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_prompt(prompt_template, review_text):\n",
                "    return prompt_template.format(review_text=review_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ba9b52dd",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "id": "08d058eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "client_or = OpenAI(\n",
                "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
                "    base_url=\"https://openrouter.ai/api/v1\"\n",
                ")\n",
                "\n",
                "def call_llm(prompt):\n",
                "    response = client_or.chat.completions.create(\n",
                "        model=\"meta-llama/llama-3.1-8b-instruct\",\n",
                "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "        temperature=0\n",
                "    )\n",
                "    return response.choices[0].message.content\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "id": "29cd18bb",
            "metadata": {},
            "outputs": [],
            "source": [
                "def parse_response(response_text):\n",
                "    # Try to find JSON if wrapped in markdown\n",
                "    match = re.search(r'{.*}', response_text, re.DOTALL)\n",
                "    json_str = match.group(0) if match else response_text\n",
                "\n",
                "    try:\n",
                "        parsed = json.loads(json_str)\n",
                "        if \"predicted_stars\" in parsed:\n",
                "            return parsed, True\n",
                "    except json.JSONDecodeError:\n",
                "        pass\n",
                "    \n",
                "    return None, False\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "92a43eab",
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_prompt(prompt_template, df_eval):\n",
                "    records = []\n",
                "\n",
                "    for _, row in df_eval.iterrows():\n",
                "        review_text = row['text']\n",
                "        actual_stars = row[\"stars\"]\n",
                "\n",
                "        prompt = build_prompt(prompt_template, review_text)\n",
                "        raw_output = call_llm(prompt) \n",
                "\n",
                "        parsed, json_valid = parse_response(raw_output) if raw_output else (None, False)\n",
                "        predicted_stars = parsed[\"predicted_stars\"] if json_valid else None\n",
                "\n",
                "        records.append({\n",
                "            \"actual_stars\": actual_stars,\n",
                "            \"predicted_stars\": predicted_stars,\n",
                "            \"json_valid\": json_valid,\n",
                "            \"raw_output\": raw_output\n",
                "        })\n",
                "\n",
                "        time.sleep(2) \n",
                "    \n",
                "    df_results = pd.DataFrame(records)\n",
                "    \n",
                "    # Calculate metrics safely\n",
                "    valid_results = df_results[df_results['json_valid'] == True]\n",
                "    accuracy = (valid_results['actual_stars'] == valid_results['predicted_stars']).mean() if len(valid_results) > 0 else 0\n",
                "    json_validity_rate = df_results['json_valid'].mean() if len(df_results) > 0 else 0\n",
                "\n",
                "    print(f\"\\nEvaluation Complete: Accuracy={accuracy:.3f}, JSON Validity Rate={json_validity_rate:.3f}\")\n",
                "\n",
                "    return df_results, accuracy, json_validity_rate\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "524474d3",
            "metadata": {},
            "outputs": [],
            "source": [
                "results_v1, acc_v1, json_v1 = evaluate_prompt(prompt_v1, df_task)\n",
                "results_v2, acc_v2, json_v2 = evaluate_prompt(prompt_v2, df_task)\n",
                "results_v3, acc_v3, json_v3 = evaluate_prompt(prompt_v3, df_task)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "70d7ec11",
            "metadata": {},
            "outputs": [],
            "source": [
                "comparison_table = pd.DataFrame([\n",
                "    {\"prompt\": \"v1_baseline\", \"accuracy\": acc_v1, \"json_validity\": json_v1},\n",
                "    {\"prompt\": \"v2_few_shot\", \"accuracy\": acc_v2, \"json_validity\": json_v2},\n",
                "    {\"prompt\": \"v3_strict\", \"accuracy\": acc_v3, \"json_validity\": json_v3},\n",
                "])\n",
                "\n",
                "comparison_table\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "discussion_sec",
            "metadata": {},
            "source": [
                "# Discussion of Results and Trade-offs\n",
                "\n",
                "## Results Analysis\n",
                "All three prompts achieved 100% JSON validity, verifying the robustness of the implementation.\n",
                "\n",
                "- **Accuracy**: **`v3_strict` performed best (0.640)**, finding a notable advantage over `v1_baseline` (0.620) and `v2_few_shot` (0.595).\n",
                "- **Observation**: The strict prompting strategy, which encourages internal component-based analysis (weighing negatives vs positives), proved superior here. It likely helped the model navigate mixed-sentiment reviews where a simple \"gut feel\" classification (V1) or a patterned few-shot approach (V2) failed.\n",
                "\n",
                "## Trade-offs\n",
                "| Prompt Type | Pros | Cons |\n",
                "|------------|------|------|\n",
                "| **V1 (Baseline)** | Low token cost, simple. Good baseline performance. | Lacks nuance for complex cases. |\n",
                "| **V2 (Few-Shot)** | Structured. Helps with format adherence. | Highest token cost. Examples can bias the model if not perfectly representative. |\n",
                "| **V3 (Strict/CoT)** | **Highest Accuracy**. Best at handling nuance and mixed feelings. | More complex prompt structure. |\n",
                "\n",
                "**Conclusion**: For this dataset, the **V3 Strict** prompt is the clear winner, justifying the slight complexity increase with superior accuracy."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "resume_ai",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
